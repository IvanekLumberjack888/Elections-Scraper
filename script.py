from __future__ import annotations
import requests                             # pro requests.get stahov√°n√≠
from bs4 import BeautifulSoup               # parsovn√≠ str√°nek
from urllib.parse import urljoin            # spojov√°n√≠ adres URL
# import os                                 # pro nap≈ô. clearov√°n√≠
import csv                                  # pro CSV
import sys                                  # z p≈ôik√°zov√© ≈ô√°dky
from typing import List, Dict, Tuple        # pro typov√°n√≠
import time                                 # pro ƒçasov√© pauzy mezi requesty

"""
    main.py: t≈ôet√≠ projekt do Engeto Online Python Akademie

    author: Ivo Dole≈æal
    email: ivousd@seznam.cz/ivousd@gmail.com

        WW      WW EEEEEEE BBBBB      SSSSS   CCCCC  RRRRRR    AAA   PPPPPP  EEEEEEE RRRRRR  
        WW      WW EE      BB   B    SS      CC    C RR   RR  AAAAA  PP   PP EE      RR   RR 
        WW   W  WW EEEEE   BBBBBB     SSSSS  CC      RRRRRR  AA   AA PPPPPP  EEEEE   RRRRRR  
         WW WWW WW EE      BB   BB        SS CC    C RR  RR  AAAAAAA PP      EE      RR  RR  
          WW   WW  EEEEEEE BBBBBB     SSSSS   CCCCC  RR   RR AA   AA PP      EEEEEEE RR   RR 
    ---
    V p≈ô√≠kazov√© ≈ô√°dce:
        -> Obecn√© pou≈æit√≠:
        python main.py <URL_okresu> <vystupni_soubor.csv>
        -> üëâ M≈Øj p≈ô√≠klad: üëà
        python main.py 'https://www.volby.cz/pls/ps2017nss/ps32?xjazyk=CZ&xkraj=11&xnumnuts=6203' 'vystup.csv'
"""

# CONSTANTS
HEADERS = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36"}
# --- IGNORE --- Z√°kladn√≠ maskov√°n√≠ prohl√≠≈æeƒçe #NEJSME_ROBOTI ü§ñü§ñü§ñ
# --- IGNORE --- NA WEBU VOLBY.CZ/ROBOTS.TXT je pouze:
# --- IGNORE --- User-agent: * a taky "Disallow: /pls/" -> tak≈æe pohoda üòâ
SLEEP = 0.8     # PAUZA mezi jednotliv√Ωmi vol√°n√≠mi get. Aby server nezkolaboval. Je to v sekund√°ch
# --- IGNORE --- Pro jistotu

# Odkaz na hlavn√≠ str√°nku s v√Ωsledky voleb do Poslaneck√© snƒõmovny ƒåR 2017
url = "https://volby.cz/pls/ps2017nss/ps3?xjazyk=CZ"

# Odkaz na okres, kter√Ω budeme zpracov√°vat (üëâ M≈Øj p≈ô√≠klad üëà)
district_url = "https://www.volby.cz/pls/ps2017nss/ps32?xjazyk=CZ&xkraj=11&xnumnuts=6203"

# V√Ωpis z√°kladn√≠ch informac√≠ o programu
print(
    f'''Skrejpujeme volebn√≠ data z vybran√©ho okresu, kter√Ω je na str√°nce {url}
    a to konkr√©tnƒõ z okresu {district_url}'''
    )

# Kontrola argument≈Ø - ƒçili url a soubor pro ulo≈æen√≠ dat
def validate_args() -> Tuple[str, str]:
    """Kontrola argument≈Ø z p≈ô√≠kazov√© ≈ô√°dky."""
    if len(sys.argv) != 3:
        print("Pou≈æit√≠: python main.py <URL_okresu> <vystupni_soubor.csv>")
        sys.exit(1)

    district_url, outputfile = sys.argv[1], sys.argv[2]

    if not district_url.startswith("http"):
        print("Zadan√° URL nen√≠ platn√°. Ujistƒõte se, ≈æe zaƒç√≠n√° na http:// nebo https://")
        sys.exit(1)

    if "volby.cz" not in district_url or "ps32" not in district_url:
        print("Zadan√° URL nen√≠ platn√°. Ujistƒõte se, ≈æe obsahuje 'volby.cz' a 'ps32'.")
        sys.exit(1)
    
    if not outputfile.endswith(".csv"):
        outputfile += ".csv"

    return district_url, outputfile

# =====================================================
# ========== Z√°kladn√≠ stahov√°n√≠ a parsov√°n√≠: ==========
# =====================================================
def fetch_data(url: str) -> str:                                                   
    """Funkce pro z√≠sk√°n√≠ HTML obsahu z dan√© URL."""
    try:
        resp = requests.get(url, headers=HEADERS, timeout=15)
        resp.raise_for_status()
        return resp.text
    except requests.RequestException as e:
        print(f"Chyba p≈ôi stahov√°n√≠ dat z {url}: {e}")
        sys.exit(1)

def make_soup(url: str) -> BeautifulSoup:                                   
    """Funkce pro vytvo≈ôen√≠ BeautifulSoup objektu z HTML obsahu."""
    html_content = fetch_data(url)
    return BeautifulSoup(html_content, features="html.parser")

def parse_h3_title(soup: BeautifulSoup) -> tuple[str, str]:
    """
    Vy≈ôe≈°√≠ opakov√°n√≠ parsov√°n√≠ n√°zvu a k√≥du obce z H3 tagu.
    """
    h3 = soup.select_one("h3")
    if h3:
        text = h3.get_text(strip=True)
        if "k√≥d" in text:
            nazev, kod = text.rsplit("k√≥d", 1)
            return nazev.replace("-", "").strip(), kod.strip()
        else:
            return text, ""
    else:
        return "", ""

# Obce z okresu
def get_municipality_links(district_url: str):
    """
    Vrac√≠ odkazy z jednotliv√Ωch okres≈Ø do listu
    """
    html_content = fetch_data(district_url)
    soup = BeautifulSoup(html_content, "html.parser")

    links = []
    for td in soup.select("td.cislo"):
        a_tag = td.select_one("a[href]")
        if a_tag:
            full_url = urljoin(district_url, a_tag["href"])
            links.append(full_url)

    print(f"Nalezeno {len(links)} obc√≠.")
    return links

# √ödaje z obce -> jednotliv√© fce

def parse_municipality_code(soup: BeautifulSoup) -> str:                   
    """
    Vrac√≠ k√≥d obce
    """
    _, kod = parse_h3_title(soup)
    return kod

def get_municipality_name(soup: BeautifulSoup) -> str:                     # get_municipality_name() -> parsov√°n√≠ n√°zvu obce
    """
    Vrac√≠ n√°zev obce
    """
    _, nazev = parse_h3_title(soup)
    return nazev

def get_municipality_stats(soup: BeautifulSoup) -> dict[str, int]:
    """
    Vrac√≠ statistiky obce (voliƒçi, vydan√© ob√°lky, platn√© hlasy)
    """ 
    stats = soup.select('td:has(span.number)')

    vysledek = {
        "voliƒçi": 0,
        "vydan√© ob√°lky": 0,
        "platn√© hlasy": 0
    }

    if len(stats) >= 3:
        try:
            vysledek["voliƒçi"] = int(stats[0].get_text().replace("\xa0", ""))
            vysledek["vydan√© ob√°lky"] = int(stats[1].get_text().replace("\xa0", ""))
            vysledek["platn√© hlasy"] = int(stats[2].get_text().replace("\xa0", ""))
        except ValueError:
        # Pokud dojde k chybƒõ, budou nuly
            pass

    return vysledek

def get_municipality_parties(soup: BeautifulSoup) -> dict[str, int]:
    """
    Vrac√≠ slovn√≠k s poƒçtem hlas≈Ø pro jednotliv√© strany v obci.
    """
    parties = {}

    for table in soup.select("table"):
        for row in table.select("tr")[2:]:
            cells = row.select("td")
            if len(cells) >= 3:
                strana = cells[1].get_text(strip=True)
                hlasy_text = cells[2].get_text(strip=True).replace("\xa0", "")
                try:
                    hlasy = int(hlasy_text)
                except ValueError:
                    hlasy = 0
                if strana:
                    parties[strana] = hlasy

    return parties

def parse_municipality_data(municipality_url: str) -> dict:
    """
    Hlavn√≠ funkce - zpracuje jednu obec pomoc√≠ men≈°√≠ch funkc√≠.
    Tohle je orchestr√°tor kter√Ω spojuje v≈°echny municipality_ funkce.
    """
    html_content = fetch_data(municipality_url)
    soup = BeautifulSoup(html_content, "html.parser")

    data = {}
    # Propojen√≠ fc√≠
    data["k√≥d obce"] = parse_municipality_code(soup)
    data["n√°zev obce"] = get_municipality_name(soup)

    # K tomu se p≈ôipoj√≠ stats
    data.update(get_municipality_stats(soup))
    # P≈ôid√°n√≠ stran
    data.update(get_municipality_parties(soup))

    return data

# =====================================================
# ====================== CSV ==========================
# =====================================================

def save_to_csv(data: List[Dict[str, str]], filename: str) -> None:
    """Ulo≈æ√≠ data do CSV souboru."""
    if not data:
        print("≈Ω√°dn√° data k ulo≈æen√≠.")
        return

    base_columns = ["k√≥d obce", "n√°zev obce", "voliƒçi v seznamu", "vydan√© ob√°lky", "platn√© hlasy"]
    with open(filename, mode="w", newline="", encoding="utf-8") as csv_file:
        fieldnames = data[0].keys()
        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(data)
        csv_file.close()

# **Hlavn√≠ ƒç√°st programu**
def main(argv: List[str] = None) -> None:
    """Hlavn√≠ funkce programu."""
    if argv is None:
        argv = sys.argv[1:]

    if len(sys.argv) != 3:
        print(f"Pou≈æit√≠: {sys.argv[0]} <url> a {sys.argv[1]} jako <soubor pro ulo≈æen√≠ dat v CSV form√°tu>")
        return
    # ...
    for i, link in enumerate(municipality_links, 1):
        # ...
        municipality_data = parse_municipality_data(link)

    if not argv:
        print("Nebyl zad√°n ≈æ√°dn√Ω soubor pro ulo≈æen√≠ dat.")
        return
    
    url = argv[0]  # Prvn√≠ argument je URL
    if not url.startswith("http"):
        print("Zadan√° URL nen√≠ platn√°. Ujistƒõte se, ≈æe zaƒç√≠n√° na http:// nebo https://")
        return
    if not url.endswith("/"):
        url += "/"  # P≈ôid√° lom√≠tko na konec URL, pokud tam nen√≠
    print(f"Stahuji data z {url}")

    # Druh√Ω argument je n√°zev souboru pro ulo≈æen√≠ dat
    if len(argv) < 2:
        print("Nebyl zad√°n n√°zev souboru pro ulo≈æen√≠ dat.")
        return
    if len(argv) > 2:
        print("Bylo zad√°no v√≠ce ne≈æ dva argumenty. Pou≈æijte pouze URL a n√°zev souboru.")
        return
    if not argv[1]:
        print("Nebyl zad√°n n√°zev souboru pro ulo≈æen√≠ dat.")
        return
    
    filename = argv[1]
    if not filename.endswith(".csv"):
        print("Zadan√Ω soubor pro ulo≈æen√≠ dat mus√≠ m√≠t p≈ô√≠ponu .csv")
    elif out_csv := filename.endswith(".csv"):
        out_csv += ".csv"  # P≈ôid√° p≈ô√≠ponu .csv, pokud nen√≠ p≈ô√≠tomna
        print(f"Ukl√°d√°m data do souboru {out_csv}")
        return
    
    # Z√≠sk√°n√≠ HTML obsahu
    soup_obj = make_soup(url)
    if not soup_obj:
        print("Nepoda≈ôilo se z√≠skat obsah str√°nky.")
        return
    # Z√≠sk√°n√≠ okres≈Ø
    districts = parse_district(soup_obj)
    
    # P≈ô√≠klad: proj√≠t v≈°echny okresy a st√°hnout jejich str√°nky s pauzou
    for district_link in districts:
        # ...zpracov√°n√≠ dat...
        time.sleep(SLEEP)  # Pauza mezi po≈æadavky

    # Ulo≈æen√≠ do CSV
    # save_to_csv(districts, filename)  # Upravte podle toho, co chcete ukl√°dat
    
    print(f"Data byla √∫spƒõ≈°nƒõ ulo≈æena do souboru {filename}")
    # V√Ωpis v≈°ech A tag≈Ø
    print("V≈°echny odkazy:")
    for link in get_all_a_tags(soup_obj):
        print(link.get_text(strip=True), link['href'])

# Hlavn√≠ funkce

if main.__name__ == "__main__":
    main()

#make_soup() - vytvo≈ô√≠ BeautifulSoup objekt
