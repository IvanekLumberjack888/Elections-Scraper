from __future__ import annotations
import requests                             # pro requests.get stahov치n칤
from bs4 import BeautifulSoup               # parsovn칤 str치nek
from urllib.parse import urljoin            # spojov치n칤 adres URL
# import os                                   # pro nap콏. clearov치n칤
import csv                                  # pro CSV
import sys                                  # z p콏ik치zov칠 콏치dky
from typing import List, Dict               # pro typov치n칤
import time                                 # pro 캜asov칠 pauzy mezi requesty

"""
    main.py: t콏et칤 projekt do Engeto Online Python Akademie

    author: Ivo Dole쬬l
    email: ivousd@seznam.cz/ivousd@gmail.com

        WW      WW EEEEEEE BBBBB      SSSSS   CCCCC  RRRRRR    AAA   PPPPPP  EEEEEEE RRRRRR  
        WW      WW EE      BB   B    SS      CC    C RR   RR  AAAAA  PP   PP EE      RR   RR 
        WW   W  WW EEEEE   BBBBBB     SSSSS  CC      RRRRRR  AA   AA PPPPPP  EEEEE   RRRRRR  
         WW WWW WW EE      BB   BB        SS CC    C RR  RR  AAAAAAA PP      EE      RR  RR  
          WW   WW  EEEEEEE BBBBBB     SSSSS   CCCCC  RR   RR AA   AA PP      EEEEEEE RR   RR 
    ---
    V p콏칤kazov칠 콏치dce:
        Obecn칠 pou쬴t칤:    python main.py <URL_okresu> <vystupni_soubor.csv>
        M콢j p콏칤klad v shell zad치코:    python main.py 'https://www.vol
    Obecn칠 pou쬴t칤:    python main.py <URL_okresu> <vystupni_soubor.csv>
    M콢j p콏칤klad v shell zad치코:   python main.py 'https://www.volby.cz/pls/ps2017nss/ps32?xjazyk=CZ&xkraj=11&xnumnuts=6203' 'vystup.csv'
    ---
"""

# **CONSTANTS**

HEADERS = {"User-Agent": "Verze 138.0.7204.184 (Ofici치ln칤 sestaven칤) (64bitov칳)"}       # Z치kladn칤 maskov치n칤 prohl칤쬰캜e #NEJSME_ROBOTI 游뱄游뱄游뱄

SLEEP = 0.8     # PAUZA mezi jednotliv칳mi vol치n칤mi get. Aby server nezkolaboval. Je to v sekund치ch

# --- IGNORE --- NA WEBU VOLBY.CZ/ROBOTS.TXT
# --- IGNORE --- User-agent: * a taky "Disallow: /pls/" -> tak쬰 pohoda 游땔

# Pou쮂셨an칳 odkaz
url = "https://volby.cz/pls/ps2017nss/ps3?xjazyk=CZ"

district_url = "https://www.volby.cz/pls/ps2017nss/ps32?xjazyk=CZ&xkraj=11&xnumnuts=6203"

# ========== Z치kladn칤 stahov치n칤 a parsov치n칤: ==========

def fetch_data(url: str) -> str:                                                         # fetch_html() -> st치hne HTML s chyt치n칤m chyb
    """Funkce pro z칤sk치n칤 HTML obsahu z dan칠 URL."""
    try:
        resp = requests.get(url, headers=HEADERS)
        resp.raise_for_status()
        return resp.text
    except requests.RequestException as e:
        print(f"Chyba p콏i stahov치n칤 dat z {url}: {e}")
        sys.exit(1) # Ukon캜칤 program





def make_soup(url: str) -> BeautifulSoup:                                                # make_soup() -> vytvo콏칤 BeautifulSoup objekt
    """Funkce pro vytvo콏en칤 BeautifulSoup objektu z HTML obsahu."""
    html_content = fetch_data(url)
    return BeautifulSoup(html_content, features="html.parser")
    
def parse_district(soup: BeautifulSoup) -> List[str]:                                    # parse_district() -> parsov치n칤 okres콢
    """
    Vrac칤 absolutn칤 odkazy z jednotliv칳ch okres콢.
    """
    return [urljoin(district_url, a_tag['href'])
            for a_tag in soup.select("td.cislo a[href]")]


def parse_obce(soup: BeautifulSoup) -> List[str]:                                        # parse_obce() -> parsov치n칤 obc칤
    """
    Vrac칤 absolutn칤 odkazy z jednotliv칳ch obc칤.
    """
    return [urljoin(district_url, a_tag['href'])
            for a_tag in soup.select("td.cislo a[href]")]


def get_all_a_tags(soup: BeautifulSoup) -> List[BeautifulSoup]:                          # get_all_a_tags() -> z칤sk치 v코echny A tagy
    """Vrac칤 v코echny A tagy v dan칠m BeautifulSoup objektu."""
    return soup.find_all("a")  

# Hlasy stran

def ziskej_hlasy_stran(soup: BeautifulSoup) -> Dict[str, str]:
    """
    Vrac칤 slovn칤k s n치zvy stran a po캜tem hlas콢 z dan칠 str치nky.
    """ 
    strany = {}
    for tabulka in tabulka.find_all("table", {"class": "table"}):  # noqa: F821
        for radek in tabulka.find_all("tr"):
            bunky = radek.find_all("td")
            if len(bunky) >= 3:
                nazev_strany = bunky[1].get_text(strip=True)
                hlasy = bunky[2].get_text(strip=True).replace('\xa0', '')
                strany[nazev_strany] = hlasy
    return strany

# **CSV**

def save_to_csv(data: List[Dict[str, str]], filename: str) -> None:
    """Ulo쮂 data do CSV souboru."""
    with open(filename, mode="w", newline="", encoding="utf-8") as csv_file:
        fieldnames = data[0].keys()
        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(data)
        csv_file.close()

# **Hlavn칤 캜치st programu**
def main(argv: List[str] = None) -> None:
    """Hlavn칤 funkce programu."""
    if argv is None:
        argv = sys.argv[1:]

    if len(sys.argv) != 3:
        print(f"Pou쬴t칤: {sys.argv[0]} <url> a {sys.argv[1]} jako <soubor pro ulo쬰n칤 dat v CSV form치tu>")
        return

    if not argv:
        print("Nebyl zad치n 쮂멳n칳 soubor pro ulo쬰n칤 dat.")
        return
    
    url = argv[0]  # Prvn칤 argument je URL
    if not url.startswith("http"):
        print("Zadan치 URL nen칤 platn치. Ujist캩te se, 쬰 za캜칤n치 na http:// nebo https://")
        return
    if not url.endswith("/"):
        url += "/"  # P콏id치 lom칤tko na konec URL, pokud tam nen칤
    print(f"Stahuji data z {url}")

    # Druh칳 argument je n치zev souboru pro ulo쬰n칤 dat
    if len(argv) < 2:
        print("Nebyl zad치n n치zev souboru pro ulo쬰n칤 dat.")
        return
    if len(argv) > 2:
        print("Bylo zad치no v칤ce ne dva argumenty. Pou쬴jte pouze URL a n치zev souboru.")
        return
    if not argv[1]:
        print("Nebyl zad치n n치zev souboru pro ulo쬰n칤 dat.")
        return
    
    filename = argv[1]
    if not filename.endswith(".csv"):
        print("Zadan칳 soubor pro ulo쬰n칤 dat mus칤 m칤t p콏칤ponu .csv")
    elif out_csv := filename.endswith(".csv"):
        out_csv += ".csv"  # P콏id치 p콏칤ponu .csv, pokud nen칤 p콏칤tomna
        print(f"Ukl치d치m data do souboru {out_csv}")
        return
    
    # Z칤sk치n칤 HTML obsahu
    soup_obj = make_soup(url)
    if not soup_obj:
        print("Nepoda콏ilo se z칤skat obsah str치nky.")
        return
    # Z칤sk치n칤 okres콢
    districts = parse_district(soup_obj)
    
    # P콏칤klad: proj칤t v코echny okresy a st치hnout jejich str치nky s pauzou
    for district_link in districts:
        # ...zpracov치n칤 dat...
        time.sleep(SLEEP)  # Pauza mezi po쬬davky

    # Ulo쬰n칤 do CSV
    # save_to_csv(districts, filename)  # Upravte podle toho, co chcete ukl치dat
    
    print(f"Data byla 칰sp캩코n캩 ulo쬰na do souboru {filename}")
    # V칳pis v코ech A tag콢
    print("V코echny odkazy:")
    for link in get_all_a_tags(soup_obj):
        print(link.get_text(strip=True), link['href'])

# Hlavn칤 funkce

if main.__name__ == "__main__":
    main()

#make_soup() - vytvo콏칤 BeautifulSoup objekt
