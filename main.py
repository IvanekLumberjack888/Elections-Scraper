from __future__ import annotations
import requests  # pro requests.get stahov√°n√≠
from bs4 import BeautifulSoup  # parsovn√≠ str√°nek
from urllib.parse import urljoin  # spojov√°n√≠ adres URL

import csv  # pro CSV
import sys  # z p≈ôik√°zov√© ≈ô√°dky
from typing import List, Dict, Tuple  # pro typov√°n√≠
import time  # pro ƒçasov√© pauzy mezi requesty

# import os                                   # pro nap≈ô. clearov√°n√≠

"""
    main.py: t≈ôet√≠ projekt do Engeto Online Python Akademie

    author: Ivo Dole≈æal
    email: ivousd@seznam.cz/ivousd@gmail.com

        WW      WW EEEEEEE BBBBB      SSSSS   CCCCC  RRRRRR    AAA   PPPPPP  EEEEEEE RRRRRR  
        WW      WW EE      BB   B    SS      CC    C RR   RR  AAAAA  PP   PP EE      RR   RR 
        WW   W  WW EEEEE   BBBBBB     SSSSS  CC      RRRRRR  AA   AA PPPPPP  EEEEE   RRRRRR  
         WW WWW WW EE      BB   BB        SS CC    C RR  RR  AAAAAAA PP      EE      RR  RR  
          WW   WW  EEEEEEE BBBBBB     SSSSS   CCCCC  RR   RR AA   AA PP      EEEEEEE RR   RR 
    ---
    VOLBY.CZ v p≈ô√≠kazov√© ≈ô√°dce:
        -> Obecn√© pou≈æit√≠:
        python main.py <URL_okresu> <vystupni_soubor.csv>
        -> üëâ M≈Øj p≈ô√≠klad ‚Äì Volby 2017 konkr√©tn√≠ v√Ωbƒõr z okresu a obc√≠ üëà
        python main.py 'https://www.volby.cz/pls/ps2017nss/ps32?xjazyk=CZ&xkraj=11&xnumnuts=6203' 'vystup.csv'
"""

# kONSTANTY
HEADERS = (
    "User-Agent", "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36"
)
# --- IGNORE --- Z√°kladn√≠ maskov√°n√≠ prohl√≠≈æeƒçe #NEJSME_ROBOTI ü§ñü§ñü§ñ
# --- IGNORE --- NA WEBU VOLBY.CZ/ROBOTS.TXT je pouze:
# --- IGNORE --- User-agent: * a taky "Disallow: /pls/" -> tak≈æe pohoda üòâ
SLEEP = 0.8  # PAUZA mezi jednotliv√Ωmi vol√°n√≠mi get. Aby server nezkolaboval. Je to v sekund√°ch
# --- IGNORE --- Pro jistotu

# Odkaz na hlavn√≠ str√°nku s v√Ωsledky voleb do Poslaneck√© snƒõmovny ƒåR 2017
url = "https://volby.cz/pls/ps2017nss/ps3?xjazyk=CZ"

# M≈Øj vybran√Ω okres
district_name = "Brno-venkov"

# Odkaz na okres
district_url = (
    "https://www.volby.cz/pls/ps2017nss/ps32?xjazyk=CZ&xkraj=11&xnumnuts=6203"
)

# V√Ωpis z√°kladn√≠ch informac√≠ o programu
print(
    f""" ...üî™+ü•î = üçü\nSkrejpujeme volebn√≠ data z vybran√©ho okresu, kter√Ω je na str√°nce:
    {url}
    A to konkr√©tnƒõ z okresu {district_name}
    Adresa je:
    {district_url}"""
)

# Kontrola argument≈Ø - ƒçili url a soubor pro ulo≈æen√≠ dat
def validate_args() -> Tuple[str, str]:
    """Kontrola argument≈Ø z p≈ô√≠kazov√© ≈ô√°dky."""
    if len(sys.argv) != 3:
        print("Pou≈æit√≠: python main.py <URL_okresu> <vystupni_soubor.csv>")
        sys.exit(1)

    district_url, outputfile = sys.argv[1], sys.argv[2]

    if not district_url.startswith("http"):
        print("Zadan√° URL nen√≠ platn√°. Ujistƒõte se, ≈æe zaƒç√≠n√° na http:// nebo https://")
        sys.exit(1)

    if "volby.cz" not in district_url or "ps32" not in district_url:
        print("Zadan√° URL nen√≠ platn√°. Ujistƒõte se, ≈æe obsahuje 'volby.cz' a 'ps32'.")
        sys.exit(1)

    if not outputfile.endswith(".csv"):
        outputfile += ".csv"

    return district_url, outputfile


# =====================================================
# ========== Z√°kladn√≠ stahov√°n√≠ a parsov√°n√≠: ==========
# =====================================================

def fetch_data(url: str) -> str:
    """Funkce pro z√≠sk√°n√≠ HTML obsahu z dan√© URL."""
    try:
        resp = requests.get(url, headers=HEADERS, timeout=15)
        resp.raise_for_status()
        return resp.text
    except requests.RequestException as e:
        print(f"Chyba p≈ôi stahov√°n√≠ dat z {url}: {e}")
        sys.exit(1)


def make_soup(url: str) -> BeautifulSoup:
    """Funkce pro vytvo≈ôen√≠ BeautifulSoup objektu z HTML obsahu."""
    html_content = fetch_data(url)
    return BeautifulSoup(html_content, features="html.parser")


def parse_h3_title(soup: BeautifulSoup) -> tuple[str, str]:
    """
    Vy≈ôe≈°√≠ opakov√°n√≠ parsov√°n√≠ n√°zvu a k√≥du obce z H3 tagu.
    """
    h3 = soup.select_one("h3")
    if h3:
        text = h3.get_text(strip=True)
        if "k√≥d" in text:
            nazev, kod = text.rsplit("k√≥d", 1)
            return nazev.replace("‚Äì", "").strip(), kod.strip()
        else:
            return text, ""
    else:
        return "", ""


# Obce z okresu
def get_municipality_links(district_url: str) -> List[str]:
    """
    Vrac√≠ odkazy z jednotliv√Ωch okres≈Ø do listu
    """
    html_content = fetch_data(district_url)
    soup = BeautifulSoup(html_content, "html.parser")

    links = []
    for td in soup.select("td.cislo"):
        a_tag = td.select_one("a[href]")
        if a_tag:
            full_url = urljoin(district_url, a_tag["href"])
            links.append(full_url)

    print(f"Nalezeno {len(links)} obc√≠.")
    return links


# √ödaje z obce -> jednotliv√© fce

def parse_municipality_code(soup: BeautifulSoup) -> str:
    """
    Vrac√≠ k√≥d obce
    """
    _, kod = parse_h3_title(soup)
    return kod


def get_municipality_name(soup: BeautifulSoup) -> str:
    """
    Vrac√≠ n√°zev obce
    """
    nazev, _ = parse_h3_title(soup)
    return nazev


def get_municipality_stats(soup: BeautifulSoup) -> Dict[str, str | int]:
    """
    Vrac√≠ statistiky obce (voliƒçi v seznamu, vydan√© ob√°lky, platn√© hlasy)
    """
    stats = soup.select("td:has(span.number)")

    vysledek = {"voliƒçi v seznamu": 0, "vydan√© ob√°lky": 0, "platn√© hlasy": 0}

    if len(stats) >= 3:
        try:
            vysledek["voliƒçi v seznamu"] = int(stats[0].get_text().replace("\xa0", ""))
            vysledek["vydan√© ob√°lky"] = int(stats[1].get_text().replace("\xa0", ""))
            vysledek["platn√© hlasy"] = int(stats[2].get_text().replace("\xa0", ""))
        except ValueError:
            # Pokud dojde k chybƒõ, budou nuly
            pass

    return vysledek


def get_municipality_parties(soup: BeautifulSoup) -> Dict[str, str | int]:
    """
    Vrac√≠ slovn√≠k s poƒçtem hlas≈Ø pro jednotliv√© strany v obci.
    """
    parties = {}

    for table in soup.select("table"):
        for row in table.select("tr")[2:]:
            cells = row.select("td")
            if len(cells) >= 3:
                strana = cells[1].get_text(strip=True)
                hlasy_text = cells[2].get_text(strip=True).replace("\xa0", "")
                try:
                    hlasy = int(hlasy_text)
                except ValueError:
                    hlasy = 0
                if strana:
                    parties[strana] = hlasy

    return parties


def parse_municipality_data(municipality_url: str) -> Dict[str, str | int]:
    """
    Hlavn√≠ funkce - zpracuje jednu obec pomoc√≠ men≈°√≠ch funkc√≠.
    Tohle je orchestr√°tor kter√Ω spojuje v≈°echny municipality_ funkce.
    """
    html_content = fetch_data(municipality_url)
    soup = BeautifulSoup(html_content, "html.parser")

    data = {}
    # Propojen√≠ fc√≠
    data["k√≥d obce"] = parse_municipality_code(soup)
    data["n√°zev obce"] = get_municipality_name(soup)

    # K tomu se p≈ôipoj√≠ stats
    data.update(get_municipality_stats(soup))
    # P≈ôid√°n√≠ stran
    data.update(get_municipality_parties(soup))

    return data


# =====================================================
# ====================== CSV ==========================
# =====================================================


def save_to_csv(data: List[Dict[str, int | str]], filename: str) -> None:
    """Ulo≈æ√≠ data do CSV souboru."""
    if not data:
        print("≈Ω√°dn√° data k ulo≈æen√≠.")
        return

    zahlavi = [
        "k√≥d obce",
        "n√°zev obce",
        "voliƒçi v seznamu",
        "vydan√© ob√°lky",
        "platn√© hlasy",
    ]

    parties = set()
    for obec in data:
        for nazev_sloupce in obec.keys():
            if nazev_sloupce not in zahlavi:
                parties.add(nazev_sloupce)
    parties = sorted(parties)
    fieldnames = zahlavi + parties

    try:
        with open(filename, mode="w", newline="", encoding="utf-8") as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames, dialect="excel-tab")
            writer.writeheader()
            writer.writerows(data)

    except IOError as e:
        print(f"Chyba p≈ôi ukl√°d√°n√≠ do CSV souboru {filename}: {e}")
        sys.exit(1)

    print(
        f"CSV ulo≈æeno: {filename} ({len(data)} z√°znam≈Ø, {len(data)} obce/obc√≠, {len(parties)} strany/stran)"
    )


# =====================================================
# ====================== MAIN =========================
# =====================================================


def main(argv: List[str] = None) -> None:
    """Hlavn√≠ funkce programu."""
    print("Election Scraper - Volby.cz 2017")
    print("=================================")

    district_url, outputfile = validate_args()

    print(f"Okres: {district_url}")
    print(f"V√Ωstup: {outputfile}\n")

    municipality_links = get_municipality_links(district_url)
    if not municipality_links:
        print("≈Ω√°dn√© obce - nenalezeno.")
        sys.exit(1)

    all_municipality_data = []
    for i, link in enumerate(municipality_links, 1):
        print(f"[{i} z {len(municipality_links)}] Zpracov√°v√°m..")
        municipality_data = parse_municipality_data(link)
        all_municipality_data.append(municipality_data)

        print(f"Zpracov√°no: {municipality_data['n√°zev obce']}, ukl√°d√°m do csv.")
        time.sleep(SLEEP)

    print("Hotovo.")
    save_to_csv(all_municipality_data, outputfile)


if __name__ == "__main__":
    main()
